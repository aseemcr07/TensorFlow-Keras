{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Week4_Exercise_Shakespeare_Answer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOwsuGQQY9OL",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "df9c816d-ed12-4878-e918-5352ec412541"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-15 17:57:30--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.187.128, 2404:6800:4008:c05::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.187.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "\r/tmp/sonnets.txt      0%[                    ]       0  --.-KB/s               \r/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-06-15 17:57:30 (76.5 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9vH8Y59ajYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d000c431-6bf2-4c85-cb10-9829ef02906b"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 100)           321100    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1605)              162105    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3211)              5156866   \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AIg2f1HBxqof",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ea1d656-9917-4cec-c192-7436eab580c2"
      },
      "source": [
        " history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 6.8970 - accuracy: 0.0213\n",
            "Epoch 2/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 6.4996 - accuracy: 0.0202\n",
            "Epoch 3/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 6.3865 - accuracy: 0.0241\n",
            "Epoch 4/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 6.2526 - accuracy: 0.0316\n",
            "Epoch 5/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 6.1558 - accuracy: 0.0372\n",
            "Epoch 6/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 6.0719 - accuracy: 0.0394\n",
            "Epoch 7/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 5.9881 - accuracy: 0.0423\n",
            "Epoch 8/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.8926 - accuracy: 0.0484\n",
            "Epoch 9/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.7897 - accuracy: 0.0556\n",
            "Epoch 10/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.6816 - accuracy: 0.0614\n",
            "Epoch 11/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.5756 - accuracy: 0.0662\n",
            "Epoch 12/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.4714 - accuracy: 0.0751\n",
            "Epoch 13/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.3740 - accuracy: 0.0796\n",
            "Epoch 14/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.2717 - accuracy: 0.0864\n",
            "Epoch 15/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.1726 - accuracy: 0.0916\n",
            "Epoch 16/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 5.0734 - accuracy: 0.0986\n",
            "Epoch 17/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.9746 - accuracy: 0.1081\n",
            "Epoch 18/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.8787 - accuracy: 0.1139\n",
            "Epoch 19/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.7815 - accuracy: 0.1222\n",
            "Epoch 20/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.6837 - accuracy: 0.1306\n",
            "Epoch 21/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.5814 - accuracy: 0.1416\n",
            "Epoch 22/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.4936 - accuracy: 0.1526\n",
            "Epoch 23/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.3902 - accuracy: 0.1636\n",
            "Epoch 24/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.2914 - accuracy: 0.1758\n",
            "Epoch 25/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.1903 - accuracy: 0.1877\n",
            "Epoch 26/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 4.0870 - accuracy: 0.2012\n",
            "Epoch 27/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.9926 - accuracy: 0.2139\n",
            "Epoch 28/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.8993 - accuracy: 0.2250\n",
            "Epoch 29/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 3.7982 - accuracy: 0.2456\n",
            "Epoch 30/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.7052 - accuracy: 0.2623\n",
            "Epoch 31/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 3.6168 - accuracy: 0.2815\n",
            "Epoch 32/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.5259 - accuracy: 0.2972\n",
            "Epoch 33/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.4377 - accuracy: 0.3186\n",
            "Epoch 34/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.3430 - accuracy: 0.3370\n",
            "Epoch 35/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.2697 - accuracy: 0.3544\n",
            "Epoch 36/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.1907 - accuracy: 0.3754\n",
            "Epoch 37/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.1137 - accuracy: 0.3908\n",
            "Epoch 38/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 3.0366 - accuracy: 0.4046\n",
            "Epoch 39/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.9745 - accuracy: 0.4218\n",
            "Epoch 40/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.9001 - accuracy: 0.4389\n",
            "Epoch 41/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.8487 - accuracy: 0.4464\n",
            "Epoch 42/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.7669 - accuracy: 0.4713\n",
            "Epoch 43/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.7109 - accuracy: 0.4811\n",
            "Epoch 44/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.6467 - accuracy: 0.4962\n",
            "Epoch 45/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.6043 - accuracy: 0.5017\n",
            "Epoch 46/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.5384 - accuracy: 0.5208\n",
            "Epoch 47/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.4957 - accuracy: 0.5318\n",
            "Epoch 48/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.4399 - accuracy: 0.5400\n",
            "Epoch 49/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.3946 - accuracy: 0.5485\n",
            "Epoch 50/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.3396 - accuracy: 0.5660\n",
            "Epoch 51/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.3027 - accuracy: 0.5699\n",
            "Epoch 52/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.2670 - accuracy: 0.5813\n",
            "Epoch 53/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.2034 - accuracy: 0.5953\n",
            "Epoch 54/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.1650 - accuracy: 0.6008\n",
            "Epoch 55/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.1250 - accuracy: 0.6128\n",
            "Epoch 56/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.0750 - accuracy: 0.6230\n",
            "Epoch 57/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 2.0456 - accuracy: 0.6251\n",
            "Epoch 58/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 2.0170 - accuracy: 0.6342\n",
            "Epoch 59/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.9869 - accuracy: 0.6398\n",
            "Epoch 60/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.9522 - accuracy: 0.6473\n",
            "Epoch 61/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.9134 - accuracy: 0.6577\n",
            "Epoch 62/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.8790 - accuracy: 0.6603\n",
            "Epoch 63/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.8439 - accuracy: 0.6693\n",
            "Epoch 64/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.8110 - accuracy: 0.6795\n",
            "Epoch 65/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.7746 - accuracy: 0.6872\n",
            "Epoch 66/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.7611 - accuracy: 0.6908\n",
            "Epoch 67/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.7417 - accuracy: 0.6907\n",
            "Epoch 68/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.6976 - accuracy: 0.7006\n",
            "Epoch 69/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.6717 - accuracy: 0.7077\n",
            "Epoch 70/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.6651 - accuracy: 0.7074\n",
            "Epoch 71/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.6280 - accuracy: 0.7162\n",
            "Epoch 72/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.6026 - accuracy: 0.7220\n",
            "Epoch 73/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.5756 - accuracy: 0.7267\n",
            "Epoch 74/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.5569 - accuracy: 0.7300\n",
            "Epoch 75/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.5403 - accuracy: 0.7342\n",
            "Epoch 76/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.5243 - accuracy: 0.7349\n",
            "Epoch 77/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.5012 - accuracy: 0.7404\n",
            "Epoch 78/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.4730 - accuracy: 0.7473\n",
            "Epoch 79/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.4684 - accuracy: 0.7452\n",
            "Epoch 80/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.4703 - accuracy: 0.7451\n",
            "Epoch 81/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.4445 - accuracy: 0.7495\n",
            "Epoch 82/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.4025 - accuracy: 0.7626\n",
            "Epoch 83/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.3874 - accuracy: 0.7619\n",
            "Epoch 84/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.3647 - accuracy: 0.7678\n",
            "Epoch 85/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.3605 - accuracy: 0.7668\n",
            "Epoch 86/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.3547 - accuracy: 0.7688\n",
            "Epoch 87/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.3469 - accuracy: 0.7652\n",
            "Epoch 88/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.3271 - accuracy: 0.7760\n",
            "Epoch 89/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.2978 - accuracy: 0.7778\n",
            "Epoch 90/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.2930 - accuracy: 0.7744\n",
            "Epoch 91/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.2795 - accuracy: 0.7798\n",
            "Epoch 92/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.2704 - accuracy: 0.7816\n",
            "Epoch 93/100\n",
            "484/484 [==============================] - 12s 26ms/step - loss: 1.2562 - accuracy: 0.7848\n",
            "Epoch 94/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.2509 - accuracy: 0.7841\n",
            "Epoch 95/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 1.2391 - accuracy: 0.7889\n",
            "Epoch 96/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 1.2294 - accuracy: 0.7858\n",
            "Epoch 97/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.2258 - accuracy: 0.7879\n",
            "Epoch 98/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.1949 - accuracy: 0.7933\n",
            "Epoch 99/100\n",
            "484/484 [==============================] - 13s 27ms/step - loss: 1.1875 - accuracy: 0.7995\n",
            "Epoch 100/100\n",
            "484/484 [==============================] - 13s 26ms/step - loss: 1.1775 - accuracy: 0.7983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljJzeIK7SRMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Vc6PHgxa6Hm",
        "colab": {}
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}